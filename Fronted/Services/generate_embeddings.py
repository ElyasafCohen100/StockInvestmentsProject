# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘         ğŸ“ Python Project ğŸ“
# â•‘
# â•‘  âœ¨ Team Members âœ¨
# â•‘
# â•‘  ğŸ§‘â€ğŸ’» Elyasaf Cohen 311557227 ğŸ§‘â€ğŸ’»
# â•‘  ğŸ§‘â€ğŸ’» Eldad Cohen   207920711 ğŸ§‘â€ğŸ’»
# â•‘  ğŸ§‘â€ğŸ’» Israel Shlomo 315130344 ğŸ§‘â€ğŸ’»
# â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# =========================================================== #
# ğŸ§  What does this file do?
# 1. Load the PDF file ğŸ“„
# 2. Clean the text ğŸ§¼
# 3. Split it into chunks âœ‚ï¸
# 4. Create an embedding vector for each chunk ğŸ”¢
# 5. Save everything to a JSON file ğŸ’¾
# 6. Build a FAISS index for later search ğŸ§ 
# =========================================================== #

# =========== Load libraries ============ #
import os
from vector_store import create_faiss_index
from embedder import create_embeddings, save_embeddings
from data_processing import load_pdf_text, clean_text, split_to_chunks

# =========== Set the path to the PDF file ============ #
pdf_path = r"C:\Users\elyas\PycharmProjects\InvestmentAdvisor\Fronted\Services\Dataset\merged_data.pdf"

# =========== Step 1: Load and clean the text ============ #
text = load_pdf_text(pdf_path)
cleaned_text = clean_text(text)

# =========== Step 2: Split text into chunks ============ #
chunks = split_to_chunks(cleaned_text)

# =========== Step 3: Create embeddings from chunks ============ #
embeddings = create_embeddings(chunks)

# =========== Step 4: Save the embeddings and chunks ============ #
output_json = os.path.join("Fronted", "Services", "Dataset", "embeddings.json")
os.makedirs(os.path.dirname(output_json), exist_ok=True)
save_embeddings(chunks, embeddings, output_json)

# =========== Step 5: Create FAISS index (optional test) ============ #
index = create_faiss_index(embeddings)

# =========== Print confirmation ============ #
print("âœ… All steps completed successfully!")
print(f"ğŸ”¢ Total chunks: {len(chunks)}")
