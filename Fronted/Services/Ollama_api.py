# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘         ğŸ“ Python Project ğŸ“
# â•‘
# â•‘  âœ¨ Team Members âœ¨
# â•‘
# â•‘  ğŸ§‘â€ğŸ’» Elyasaf Cohen 311557227 ğŸ§‘â€ğŸ’»
# â•‘  ğŸ§‘â€ğŸ’» Eldad Cohen   207920711 ğŸ§‘â€ğŸ’»
# â•‘  ğŸ§‘â€ğŸ’» Israel Shlomo 315130344 ğŸ§‘â€ğŸ’»
# â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘     ğŸ¤– OLlama API Integration File ğŸ¤–          â•‘
# â•‘                                                â•‘
# â•‘  This file defines a helper function to send   â•‘
# â•‘  a prompt to a local Ollama model (like Mistralâ•‘
# â•‘  or LLaMA) and return its response.            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import requests  # For sending http requests via internet or to local service


# ================================================================================= #
# A function that sends a text prompt to a locally running LLM model with Ollama,   #
# and returns the response we got back ğŸ“©                                           #
# ================================================================================= #
import requests

def ask_ollama(prompt: str, model="gemma:2b") -> str:
    try:
        res = requests.post("http://localhost:11434/api/generate", json={
            "model": model,
            "prompt": prompt,
            "stream": False
        })
        data = res.json()

        # × ×‘×“×•×§ ×× ×™×© ×©×“×” ×©×œ ×©×’×™××”
        if "error" in data:
            return f"âŒ Ollama Error: {data['error']}"

        # × ×‘×“×•×§ ×× ×™×© ×‘×›×œ×œ ×©×“×” ×‘×©× response
        if "response" not in data:
            return f"âŒ Unexpected reply format: {data}"

        return data["response"]

    except Exception as e:
        return f"âŒ Exception: {e}"

